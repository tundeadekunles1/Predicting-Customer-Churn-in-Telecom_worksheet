{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfba655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "# Make project root importable\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_training_edited import prepare_data, train_logistic_regression, train_random_forest\n",
    "from src.evaluation_edited import evaluate_model, recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = os.path.join(project_root, \"data\", \"processed\", \"telco_churn_processed.csv\")\n",
    "df = pd.read_csv(processed_path)\n",
    "print(\"df shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4561bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, preprocessor = prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a694df",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = train_logistic_regression(X_train, y_train, preprocessor)\n",
    "rf_model  = train_random_forest(X_train, y_train, preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b27b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_results = evaluate_model(log_model, X_test, y_test)\n",
    "rf_results  = evaluate_model(rf_model,  X_test, y_test)\n",
    "\n",
    "print(\"Logistic accuracy:\", log_results[\"accuracy\"])\n",
    "print(\"Random Forest accuracy:\", rf_results[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d166a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_results[\"classification_report\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "print(\"log_model:\", type(log_model))\n",
    "print(\"rf_model :\", type(rf_model))\n",
    "\n",
    "print(\"log_results keys:\", log_results.keys())\n",
    "print(\"rf_results  keys:\", rf_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b788195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix heatmap (Logistic Regression)\n",
    "cm = log_results[\"confusion_matrix\"]\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d797e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve comparison (use models directly for probabilities)\n",
    "y_prob_log = log_model.predict_proba(X_test)[:, 1]\n",
    "y_prob_rf  = rf_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_log, tpr_log, _ = roc_curve(y_test, y_prob_log)\n",
    "roc_auc_log = auc(fpr_log, tpr_log)\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c987e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitted pipeline already includes the preprocessor and classifier\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Get fitted preprocessor\n",
    "preprocessor_fitted = rf_model.named_steps[\"preprocessor\"]\n",
    "\n",
    "# 3. Get all feature names using get_feature_names_out\n",
    "# This works for both numeric and categorical features in a ColumnTransformer\n",
    "try:\n",
    "    feature_names = preprocessor_fitted.get_feature_names_out()\n",
    "except:\n",
    "    # fallback: combine numeric + categorical manually\n",
    "    num_features = preprocessor_fitted.transformers_[0][2]\n",
    "    cat_pipeline = preprocessor_fitted.transformers_[1][1]\n",
    "    cat_encoder = cat_pipeline.named_steps[\"encoder\"]\n",
    "    cat_features = cat_encoder.get_feature_names_out(cat_pipeline.feature_names_in_)\n",
    "    feature_names = list(num_features) + list(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.named_steps[\"classifier\"].feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x=importances[indices],\n",
    "    y=[feature_names[i] for i in indices],\n",
    "    hue=[feature_names[i] for i in indices],  # assign y to hue\n",
    "    palette=\"viridis\",\n",
    "    dodge=False,   # keep bars together\n",
    "    legend=False   # remove legend\n",
    ")\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9403f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr_log, tpr_log, color=\"blue\",  label=f\"Logistic Regression (AUC = {roc_auc_log:.2f})\")\n",
    "plt.plot(fpr_rf,  tpr_rf,  color=\"green\", label=f\"Random Forest (AUC = {roc_auc_rf:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_pr(y_true, y_prob):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_pr(y_test, y_prob_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_prob_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3fa29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision-Recall thresholds:\", thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b288774",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956be1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thresholds[1]) # → gives you the cutoff\n",
    "\n",
    "print(precision[1])\n",
    "print(recall[1]) # → what happens at that cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29366a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_adj = precision[:-1]\n",
    "recall_adj = recall[:-1]\n",
    "\n",
    "mask = (recall_adj >= 0.5) & (precision_adj >= 0.6)\n",
    "\n",
    "optimal_threshold = thresholds[mask][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ded5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_20_log = recall_at_k(y_test, y_prob_log, k=0.2)\n",
    "print(\"Logistic Regression Recall at 20%:\", recall_20_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb853a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_20_rf = recall_at_k(y_test, y_prob_rf, k=0.2)\n",
    "print(\"Random Forest Recall at 20%:\", recall_20_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d96153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIFT METRIC CALCULATION\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def lift_at_k(y_true, y_prob, k=0.2):\n",
    "    \"\"\"Return (precision_at_k, baseline_rate, lift_at_k) for top-k ranked customers.\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_prob = np.asarray(y_prob)\n",
    "\n",
    "    n = y_true.shape[0]\n",
    "    k_n = max(1, int(np.ceil(k * n)))\n",
    "\n",
    "    top_idx = np.argsort(y_prob)[::-1][:k_n]\n",
    "\n",
    "    precision_k = y_true[top_idx].mean()  \n",
    "    baseline = y_true.mean()  \n",
    "    lift = precision_k / baseline if baseline > 0 else np.nan\n",
    "\n",
    "    return precision_k, baseline, lift\n",
    "\n",
    "\n",
    "for k in [0.10, 0.20]:\n",
    "    p_log, base, lift_log = lift_at_k(y_test, y_prob_log, k=k)\n",
    "    p_rf, base, lift_rf = lift_at_k(y_test, y_prob_rf, k=k)\n",
    "\n",
    "    print(\n",
    "        f\"\\n--- Lift@{int(k * 100)}% (Top {int(k * 100)}% highest-risk customers) ---\"\n",
    "    )\n",
    "    print(f\"Baseline churn rate (random): {base:.3f}\")\n",
    "    print(\n",
    "        f\"Logistic: Precision@{int(k * 100)}% = {p_log:.3f}  | Lift = {lift_log:.2f}x\"\n",
    "    )\n",
    "    print(f\"RF      : Precision@{int(k * 100)}% = {p_rf:.3f}  | Lift = {lift_rf:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d05c64d",
   "metadata": {},
   "source": [
    "SHAP Explainability for Logistic Regression and Random Forest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def func_split_pipeline(pipeline):\n",
    "    \"\"\"Return (preprocessor, estimator) from a sklearn Pipeline-like object.\"\"\"\n",
    "    if hasattr(pipeline, \"named_steps\") and hasattr(pipeline, \"steps\"):\n",
    "        steps = pipeline.named_steps\n",
    "\n",
    "        pre = steps.get(\"preprocessor\", None)\n",
    "\n",
    "        if pre is None:\n",
    "            for name, step in pipeline.steps[:-1]:\n",
    "                if hasattr(step, \"transform\"):\n",
    "                    pre = step\n",
    "                    break\n",
    "\n",
    "        est = pipeline.steps[-1][1]\n",
    "        return pre, est\n",
    "\n",
    "    return None, pipeline\n",
    "\n",
    "\n",
    "def func_get_feature_names(pre, X):\n",
    "    \"\"\"Best-effort feature name recovery after preprocessing.\"\"\"\n",
    "    if pre is not None and hasattr(pre, \"get_feature_names_out\"):\n",
    "        try:\n",
    "            return np.array(pre.get_feature_names_out(), dtype=str)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if hasattr(X, \"columns\"):\n",
    "        return np.array(X.columns, dtype=str)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def func_transform(pre, X):\n",
    "    \"\"\"Apply preprocessor transform if present; otherwise return X.\"\"\"\n",
    "    if pre is None:\n",
    "        return X\n",
    "    return pre.transform(X)\n",
    "\n",
    "\n",
    "def func_sample_rows(X, n=2000, random_state=42):\n",
    "    \"\"\"Sample rows from DataFrame / ndarray / sparse matrix.\"\"\"\n",
    "    n = min(n, X.shape[0])\n",
    "\n",
    "    if hasattr(X, \"sample\"):\n",
    "        Xs = X.sample(n=n, random_state=random_state)\n",
    "        idx = Xs.index\n",
    "        return Xs, idx\n",
    "\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    idx = rng.choice(np.arange(X.shape[0]), size=n, replace=False)\n",
    "    return X[idx], idx\n",
    "\n",
    "\n",
    "def func_align_y(y, idx):\n",
    "    \"\"\"Align y to sampled indices (works for pandas Series or numpy arrays).\"\"\"\n",
    "    if hasattr(y, \"loc\"):\n",
    "        return y.loc[idx]\n",
    "    return y[idx]\n",
    "\n",
    "\n",
    "def func_select_positive_class(shap_exp):\n",
    "    \"\"\"\n",
    "    Ensure we explain the positive class (class=1) for classification outputs.\n",
    "    Some SHAP versions return (n_samples, n_features, n_classes).\n",
    "    \"\"\"\n",
    "    if hasattr(shap_exp, \"values\") and getattr(shap_exp.values, \"ndim\", 0) == 3:\n",
    "        return shap_exp[..., 1]\n",
    "    return shap_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For LOGISTIC REGRESSION\n",
    "\n",
    "pre_log, est_log = func_split_pipeline(log_model)\n",
    "if \"preprocessor\" in globals() and preprocessor is not None:\n",
    "    pre_log = preprocessor\n",
    "\n",
    "X_test_s, idx_s = func_sample_rows(X_test, n=2000, random_state=42)\n",
    "y_test_s = func_align_y(y_test, idx_s)\n",
    "\n",
    "X_train_t = func_transform(pre_log, X_train)\n",
    "X_test_t  = func_transform(pre_log, X_test_s)\n",
    "\n",
    "feature_names = func_get_feature_names(pre_log, X_train)\n",
    "\n",
    "try:\n",
    "    background = shap.utils.sample(X_train_t, 200, random_state=42)\n",
    "except Exception:\n",
    "    background = X_train_t\n",
    "\n",
    "explainer_log = shap.Explainer(est_log, background, feature_names=feature_names)\n",
    "\n",
    "shap_values_log = explainer_log(X_test_t)\n",
    "shap_values_log = func_select_positive_class(shap_values_log)\n",
    "\n",
    "print(\"Logistic SHAP shape:\", shap_values_log.values.shape)\n",
    "\n",
    "try:\n",
    "    shap.plots.bar(shap_values_log, max_display=20)\n",
    "    shap.plots.beeswarm(shap_values_log, max_display=20)\n",
    "except Exception:\n",
    "    X_plot = X_test_t.toarray() if (sp is not None and sp.issparse(X_test_t)) else X_test_t\n",
    "    shap.summary_plot(shap_values_log.values, X_plot, feature_names=feature_names, max_display=20)\n",
    "\n",
    "try:\n",
    "    shap.plots.waterfall(shap_values_log[0], max_display=20)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5cbeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "pre_rf, est_rf = func_split_pipeline(rf_model)\n",
    "\n",
    "X_test_s, idx_s = func_sample_rows(X_test, n=2000, random_state=42)\n",
    "y_test_s = func_align_y(y_test, idx_s)\n",
    "\n",
    "X_train_t = func_transform(pre_rf, X_train)\n",
    "X_test_t = func_transform(pre_rf, X_test_s)\n",
    "\n",
    "feature_names = func_get_feature_names(pre_rf, X_train)\n",
    "\n",
    "def _to_dense(X):\n",
    "    if sp is not None and sp.issparse(X):\n",
    "        return X.toarray()\n",
    "    return X\n",
    "\n",
    "\n",
    "X_train_bg = X_train_t\n",
    "try:\n",
    "    bg_idx = np.random.RandomState(42).choice(\n",
    "        X_train_bg.shape[0], size=min(200, X_train_bg.shape[0]), replace=False\n",
    "    )\n",
    "    background = _to_dense(X_train_bg[bg_idx])\n",
    "except Exception:\n",
    "    background = _to_dense(X_train_bg)\n",
    "\n",
    "X_test_dense = _to_dense(X_test_t)\n",
    "\n",
    "try:\n",
    "    explainer_rf = shap.TreeExplainer(\n",
    "        est_rf, data=background, feature_names=feature_names, model_output=\"probability\"\n",
    "    )\n",
    "except TypeError:\n",
    "    explainer_rf = shap.TreeExplainer(\n",
    "        est_rf, data=background, feature_names=feature_names\n",
    "    )\n",
    "\n",
    "shap_vals = explainer_rf.shap_values(X_test_dense)\n",
    "\n",
    "if isinstance(shap_vals, list):\n",
    "    shap_vals_pos = shap_vals[1]  # class 1\n",
    "    base_val = (\n",
    "        explainer_rf.expected_value[1]\n",
    "        if isinstance(explainer_rf.expected_value, (list, np.ndarray))\n",
    "        else explainer_rf.expected_value\n",
    "    )\n",
    "elif hasattr(shap_vals, \"ndim\") and shap_vals.ndim == 3:\n",
    "    shap_vals_pos = shap_vals[:, :, 1]\n",
    "    base_val = (\n",
    "        explainer_rf.expected_value[1]\n",
    "        if isinstance(explainer_rf.expected_value, (list, np.ndarray))\n",
    "        else explainer_rf.expected_value\n",
    "    )\n",
    "else:\n",
    "    shap_vals_pos = shap_vals\n",
    "    base_val = explainer_rf.expected_value\n",
    "\n",
    "\n",
    "if np.isscalar(base_val):\n",
    "    base_values_rf = np.full(X_test_dense.shape[0], base_val)\n",
    "else:\n",
    "    base_values_rf = base_val\n",
    "\n",
    "shap_values_rf = shap.Explanation(\n",
    "    values=shap_vals_pos,\n",
    "    base_values=base_values_rf,\n",
    "    data=X_test_dense,\n",
    "    feature_names=feature_names,\n",
    ")\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_vals_pos,\n",
    "    X_test_dense,\n",
    "    feature_names=feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    max_display=20,\n",
    ")\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_vals_pos, X_test_dense, feature_names=feature_names, max_display=20\n",
    ")\n",
    "\n",
    "try:\n",
    "    exp0 = shap.Explanation(\n",
    "        values=shap_vals_pos[0],\n",
    "        base_values=base_val,\n",
    "        data=X_test_dense[0],\n",
    "        feature_names=feature_names,\n",
    "    )\n",
    "    shap.plots.waterfall(exp0, max_display=20)\n",
    "except Exception as e:\n",
    "    print(\"Waterfall plot skipped due to:\", repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f71c3",
   "metadata": {},
   "source": [
    "Comparing the output of SHAP for Logistic Regression vs Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b29d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mean_abs_shap(shap_exp):\n",
    "    vals = shap_exp.values\n",
    "    return np.mean(np.abs(vals), axis=0)\n",
    "\n",
    "log_importance = cal_mean_abs_shap(shap_values_log)\n",
    "rf_importance  = cal_mean_abs_shap(shap_values_rf)\n",
    "\n",
    "if feature_names is None:\n",
    "    feature_names = np.array([f\"feature_{i}\" for i in range(len(log_importance))])\n",
    "\n",
    "top_n = 15\n",
    "top_log_idx = np.argsort(log_importance)[::-1][:top_n]\n",
    "top_rf_idx  = np.argsort(rf_importance)[::-1][:top_n]\n",
    "\n",
    "print(\"Top features (Logistic Regression):\")\n",
    "for i in top_log_idx:\n",
    "    print(f\"  {feature_names[i]}: {log_importance[i]:.4f}\")\n",
    "\n",
    "print(\"\\nTop features (Random Forest):\")\n",
    "for i in top_rf_idx:\n",
    "    print(f\"  {feature_names[i]}: {rf_importance[i]:.4f}\")\n",
    "Top features (Logistic Regression):\n",
    "  num__charges_ratio: 0.9701\n",
    "  num__MonthlyCharges: 0.7642\n",
    "  num__tenure: 0.3899\n",
    "  num__SeniorCitizen: 0.1987\n",
    "  num__TotalCharges: 0.1745\n",
    "  num__HighSpender: 0.0134\n",
    "  num__HighChurnRisk: 0.0021\n",
    "\n",
    "Top features (Random Forest):\n",
    "  num__MonthlyCharges: 0.0854\n",
    "  num__charges_ratio: 0.0796\n",
    "  num__tenure: 0.0623\n",
    "  num__TotalCharges: 0.0393\n",
    "  num__SeniorCitizen: 0.0200\n",
    "  num__HighSpender: 0.0195\n",
    "  num__HighChurnRisk: 0.0037"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
